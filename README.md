
# ğŸ¡ Prediction Interval Competition II: House Price (Kaggle 2025)

<div align="center">
  <a href="https://www.kaggle.com/competitions/prediction-interval-competition-ii-house-price">
    <img src="https://img.shields.io/badge/Kaggle-Competition-blue" alt="Kaggle Badge">
  </a>
  <a href="LICENSE">
    <img src="https://img.shields.io/badge/License-MIT-green" alt="MIT Badge">
  </a>
</div>



## ğŸ“Œ Overview

This repository contains our solution to the Kaggle competition: **Prediction Interval Competition II: House Price**, where the goal is to predict **a 90% prediction interval** for house sale prices â€” not a single point estimate. The narrower and more accurate the interval, the better.

- ğŸ“… **Start**: Apr 23, 2025
- ğŸ“… **End**: Jul 27, 2025
- ğŸ† **Evaluation Metric**: [Winkler Score](https://en.wikipedia.org/wiki/Prediction_interval#Winkler_score)
- ğŸ¯ **Goal**: Narrowest prediction intervals while maintaining 90% coverage

## ğŸ“‚ Folder Structure

```
.
â”œâ”€â”€ processed/               # Processed data (numpy arrays, encoders, scalers)
â”œâ”€â”€ models/                  # Trained models (LightGBM)
â”œâ”€â”€ datasets/                # Base datasets provided by the author
â”œâ”€â”€ src/                     # Python scripts and notebooks (modularized code)
â”œâ”€â”€ submission/              # Final submission CSVs
â”œâ”€â”€ report.pdf               # Final project report (Vietnamese)
â”œâ”€â”€ Slides.pdf               # Presentation slides
â””â”€â”€ README.md                # You're here!
```

## ğŸ§  Methodology

Our approach consists of two key components:

### 1. LightGBM Quantile Regression

We trained two separate LightGBM models to predict the 5th and 95th percentiles of the house price distribution:

`Å·_lower = Q_0.05(y|x),   Å·_upper = Q_0.95(y|x)`


This gives us a raw 90% prediction interval.

### 2. Conformal Prediction Adjustment

We then post-process the raw intervals using **Conformal Prediction**:

`residual_i = max(0, Å·_i^(l) - y_i) + max(0, y_i - Å·_i^(u))`

`q_hat = Quantile_0.9(residuals)`

`Å·_i_adj^(l) = Å·_i^(l) - q_hat`
`Å·_i_adj^(u) = Å·_i^(u) + q_hat`


This ensures the **coverage guarantee** without assuming any distribution.

## ğŸ“ˆ Performance Metrics

| Metric              | Value        |
|---------------------|--------------|
| Coverage            | 0.900        |
| Winkler Score       | 341,850.64   |
| Mean Interval Width | 247,135.10   |
| MAE                 | 63,012.56    |
| RMSE                | 116,814.94   |

## ğŸ’¡ Key Contributions

- âœ… Robust feature engineering: interactions, ratios, log-transforms, spatial features
- âœ… Effective handling of categorical, temporal, and missing data
- âœ… Modular training pipeline with support for LightGBM and Conformal Prediction
- âœ… Submission-ready architecture and reproducibility

## ğŸ§‘â€ğŸ’» Team Members

| Name                  | Student ID   | Role                             |
|-----------------------|--------------|----------------------------------|
| LÃª Äáº¡i HÃ²a            | 22120108     | Project Leader, Slides, Demo     |
| LÃª ChÃ¢u Há»¯u Thá»       | 22120350     | EDA, Data Processing             |
| Nguyá»…n TÆ°á»ng BÃ¡ch Há»·  | 22120455     | Conformal Prediction, Analysis, Training   |
| LÃª HoÃ ng VÅ©           | 22120461     | Modeling, Training, Report       |


## ğŸ“ Resources

- ğŸ“„ [Final Report (Vietnamese)](./report.pdf)
- ğŸ–¼ï¸ [Presentation Slides](./slides.pdf)

## ğŸ§ª Local Evaluation (Winkler Score)

```python
def winkler_score(y_true, l, u, alpha=0.1):
    width = u - l
    penalty = np.where(y_true < l, (2/alpha)*(l - y_true),
              np.where(y_true > u, (2/alpha)*(y_true - u), 0))
    return width + penalty
```

## ğŸ“œ License

This repository is licensed under the [MIT License](./LICENSE).

## ğŸ¤ Acknowledgements

We would like to thank the course instructors and Kaggle organizers for this wonderful opportunity to explore **uncertainty quantification** in machine learning!
